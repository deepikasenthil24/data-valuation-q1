{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness under Correlation Shifts\n",
    "\n",
    "### This Jupyter Notebook simulates the proposed pre-processing approach on the synthetic data.\n",
    "\n",
    "We consider two scenarios: supporting (1) a single metric (DP) and (2) multiple metrics (DP & EO).\n",
    "\n",
    "We use FairBatch [Roh et al., ICLR 2021] as an in-processing approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:04:54.062589Z",
     "start_time": "2024-11-03T08:04:49.931702Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import math\n",
    "import random \n",
    "import itertools\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import torch\n",
    "\n",
    "from models import LogisticRegression, weights_init_normal\n",
    "from FairBatchSampler_Multiple import FairBatch, CustomDataset\n",
    "from utils import correlation_reweighting, datasampling, test_model\n",
    "\n",
    "import cvxopt\n",
    "import cvxpy as cp\n",
    "from cvxpy import OPTIMAL, Minimize, Problem, Variable, quad_form # Work in YJ kernel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:04:55.308572Z",
     "start_time": "2024-11-03T08:04:54.110026Z"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('../../')\n",
    "from load_dataset import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process the data\n",
    "\n",
    "In the synthetic_data directory, there are a total of 6 numpy files including training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:05:11.430653Z",
     "start_time": "2024-11-03T08:05:11.413772Z"
    }
   },
   "outputs": [],
   "source": [
    "from API_Design_a import MissingValueError, SamplingError, LabelError, Injector\n",
    "# create pattern function given subpopulation\n",
    "def create_pattern(col_list, lb_list, ub_list):\n",
    "    # Check if inputs are valid\n",
    "    try:\n",
    "        assert len(col_list) == len(lb_list) == len(ub_list)\n",
    "    except:\n",
    "        print(col_list, lb_list, ub_list)\n",
    "        raise SyntaxError\n",
    "\n",
    "    def pattern(data_X, data_y):\n",
    "        # Initialize a mask of all True values\n",
    "        mask = np.ones(len(data_X), dtype=bool)\n",
    "\n",
    "        # Iterate over each condition in col_list, lb_list, and ub_list\n",
    "        for col, lb, ub in zip(col_list, lb_list, ub_list):\n",
    "            if col == 'Y':\n",
    "                mask &= (data_y >= lb) & (data_y <= ub)\n",
    "            else:\n",
    "                mask &= (data_X[col] >= lb) & (data_X[col] <= ub)\n",
    "\n",
    "        # Convert Boolean mask to binary indicators (1 for True, 0 for False)\n",
    "        binary_indicators = mask.astype(int)\n",
    "        \n",
    "        return binary_indicators\n",
    "\n",
    "    return pattern\n",
    "\n",
    "# lb_list = [0, 0]\n",
    "# ub_list = [0, 0]\n",
    "# X_train, X_test, y_train, y_test = load(dataset)\n",
    "# X_train_orig, X_test_orig = X_train.copy(), X_test.copy()\n",
    "\n",
    "# X_train_orig.reset_index(drop=True, inplace=True)\n",
    "# X_test_orig.reset_index(drop=True, inplace=True)\n",
    "# y_train.reset_index(drop=True, inplace=True)\n",
    "# y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# mv_pattern = create_pattern(['race', 'Y'], lb_list, ub_list)\n",
    "# mv_pattern_len = np.sum(mv_pattern(X_train_orig, y_train))\n",
    "# mv_num = min(mv_pattern_len, int(0.4*len(X_train_orig)))\n",
    "# mv_err = MissingValueError(list(X_train_orig.columns).index('race'), mv_pattern, mv_num / mv_pattern_len)\n",
    "\n",
    "# injecter = Injector(error_seq=[mv_err])\n",
    "# dirty_X_train_orig, dirty_y_train, _, _ = injecter.inject(X_train_orig.copy(), y_train.copy(), \n",
    "#                                                           X_train_orig, y_train, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:34:24.820791Z",
     "start_time": "2024-11-03T08:34:24.815274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1681"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mv_pattern_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T20:42:07.768033Z",
     "start_time": "2024-11-03T20:42:07.154104Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load('adult')\n",
    "\n",
    "lb_list = [0, 6, 1]\n",
    "ub_list = [0, 7, 1]\n",
    "mv_pattern = create_pattern(['gender', 'workclass', 'Y'], lb_list, ub_list)\n",
    "mv_pattern_len = np.sum(mv_pattern(X_train, y_train))\n",
    "poi_ratio = 0.1\n",
    "\n",
    "# lb_list = [2, 1, 1, 0]\n",
    "# ub_list = [9, 1, 1, 0]\n",
    "# mv_pattern = create_pattern(['education', 'gender', 'relationship', 'Y'], lb_list, ub_list)\n",
    "# mv_pattern_len = np.sum(mv_pattern(X_train, y_train))\n",
    "# poi_ratio = 0.2\n",
    "\n",
    "mv_num = min(mv_pattern_len, int(poi_ratio*len(X_train)))\n",
    "mv_err = SamplingError(mv_pattern, mv_num / mv_pattern_len)\n",
    "injector = Injector(error_seq=[mv_err])\n",
    "X_train, y_train, _, _ = injector.inject(X_train, y_train, X_train, y_train, seed=0)\n",
    "\n",
    "y_train = y_train.replace({0: -1, 1: 1})\n",
    "y_test = y_test.replace({0: -1, 1: 1})\n",
    "\n",
    "xz_train = X_train.copy()\n",
    "z_train = X_train.gender.copy()\n",
    "y_noise = y_train.copy()\n",
    "\n",
    "xz_test = X_test.copy()\n",
    "z_test = X_test.gender.copy()\n",
    "\n",
    "# y_train = y_train*2-1\n",
    "# y_test = y_test*2-1\n",
    "\n",
    "xz_train = torch.FloatTensor(xz_train.to_numpy())\n",
    "y_train = torch.FloatTensor(y_train.to_numpy())\n",
    "z_train = torch.FloatTensor(z_train.to_numpy())\n",
    "\n",
    "xz_test = torch.FloatTensor(xz_test.to_numpy())\n",
    "y_test = torch.FloatTensor(y_test.to_numpy())\n",
    "z_test = torch.FloatTensor(z_test.to_numpy())\n",
    "\n",
    "# os.chdir('robust_algorithms/fair-robust-selection')\n",
    "# xz_train = np.load('./synthetic_data/xz_train.npy')\n",
    "# y_train = np.load('./synthetic_data/y_train.npy')\n",
    "# z_train = np.load('./synthetic_data/z_train.npy')\n",
    "# \n",
    "# y_noise = np.load('./synthetic_data/y_noise_general.npy') # Labels with the general label flipping (details are in the paper)\n",
    "# poi_ratio = 0.0\n",
    "# \n",
    "# xz_test = np.load('./synthetic_data/xz_test.npy')\n",
    "# y_test = np.load('./synthetic_data/y_test.npy') \n",
    "# z_test = np.load('./synthetic_data/z_test.npy')\n",
    "# \n",
    "# xz_train = torch.FloatTensor(xz_train)\n",
    "# y_train = torch.FloatTensor(y_train)\n",
    "# z_train = torch.FloatTensor(z_train)\n",
    "# \n",
    "# y_noise = torch.FloatTensor(y_noise)\n",
    "# \n",
    "# xz_test = torch.FloatTensor(xz_test)\n",
    "# y_test = torch.FloatTensor(y_test)\n",
    "# z_test = torch.FloatTensor(z_test)\n",
    "# os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T20:42:08.083403Z",
     "start_time": "2024-11-03T20:42:08.075474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "801"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mv_pattern_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T20:42:09.922544Z",
     "start_time": "2024-11-03T20:42:09.263468Z"
    }
   },
   "outputs": [],
   "source": [
    "seeds = [0,1,2,3,4]\n",
    "\n",
    "w = np.array([sum((z_train==1)&(y_train==1))/len(y_train), sum((z_train==0)&(y_train==1))/len(y_train), sum((z_train==1)&(y_train==-1))/len(y_train), sum((z_train==0)&(y_train==-1))/len(y_train)])\n",
    "corr = 0.18\n",
    "alpha = 0.005 # Used in FairBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T20:42:09.935916Z",
     "start_time": "2024-11-03T20:42:09.930982Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_epoch(model, train_features, labels, optimizer, criterion):\n",
    "    \"\"\"Trains the model with the given train data.\n",
    "\n",
    "    Args:\n",
    "        model: A torch model to train.\n",
    "        train_features: A torch tensor indicating the train features.\n",
    "        labels: A torch tensor indicating the true labels.\n",
    "        optimizer: A torch optimizer.\n",
    "        criterion: A torch criterion.\n",
    "\n",
    "    Returns:\n",
    "        loss value.\n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    label_predicted = model.forward(train_features)\n",
    "    loss  = criterion((F.tanh(label_predicted.squeeze())+1)/2, (labels.squeeze()+1)/2)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T20:42:09.980899Z",
     "start_time": "2024-11-03T20:42:09.968668Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_w_cvxpy(w, corr, gamma1, gamma2):\n",
    "    \n",
    "    \"\"\"Solves the SDP relaxation problem.\n",
    "\n",
    "    Args:\n",
    "        w: A list indicating the original data ratio for each (y, z)-class.\n",
    "        corr: A real number indicating the target correlation.\n",
    "        gamma1: A real number indicating the range of Pr(y) change\n",
    "        gamma2: A real number indicating the range of Pr(z) change\n",
    "\n",
    "    Returns:\n",
    "        solution for the optimization problem.\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(w)\n",
    "    a = w[0]\n",
    "    b = w[1]\n",
    "    c = w[2]\n",
    "    d = w[3]\n",
    "    orig_corr = w[0]/(w[0]+w[2]) - w[1]/(w[1]+w[3])\n",
    "\n",
    "    P0 = np.array([[1,0,0,0,-a],[0,1,0,0,-b],[0,0,1,0,-c],[0,0,0,1,-d],[-a,-b,-c,-d,0]])\n",
    "    \n",
    "    P1 = np.array([[0,-corr/2,0,(1-corr)/2,0],[-corr/2,0,(-1-corr)/2,0,0],[0,(-1-corr)/2,0,-corr/2,0],[(1-corr)/2,0,-corr/2,0,0],[0,0,0,0,0]])\n",
    "\n",
    "    P2 = np.array([[0,0,0,0,1],[0,0,0,0,1],[0,0,0,0,0],[0,0,0,0,0],[1,1,0,0,0]])\n",
    "    r2 = -2*(a+b)\n",
    "\n",
    "    P3 = np.array([[0,0,0,0,1],[0,0,0,0,0],[0,0,0,0,1],[0,0,0,0,0],[1,0,1,0,0]])\n",
    "    r3 = -2*(a+c)\n",
    "\n",
    "    P4 = np.array([[0,0,0,0,1],[0,0,0,0,1],[0,0,0,0,1],[0,0,0,0,1],[1,1,1,1,0]])\n",
    "    r4 = -2*1\n",
    "\n",
    "    P5 = np.array([[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,1]])\n",
    "\n",
    "    X = cp.Variable((n+1,n+1), symmetric=True)\n",
    "\n",
    "    constraints = [X >> 0]\n",
    "    constraints = [\n",
    "        cp.trace(P1 @ X) == 0,\n",
    "        cp.trace(P2 @ X) + r2 <= gamma1,\n",
    "        cp.trace(P2 @ X) + r2 >= -gamma1,\n",
    "        cp.trace(P3 @ X) + r3 <= gamma2,\n",
    "        cp.trace(P3 @ X) + r3 >= -gamma2,\n",
    "        cp.trace(P4 @ X) + r4 == 0,\n",
    "        cp.trace(P5 @ X) == 1,\n",
    "        X >> 0\n",
    "    ]\n",
    "    prob = cp.Problem(cp.Minimize(cp.trace(P0 @ X)),constraints)\n",
    "\n",
    "    result = prob.solve()\n",
    "\n",
    "    x = X.value\n",
    "    x = x[:, -1][:-1]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Supporting a single metric (DP)\n",
    "### The results are in the experiments of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T20:42:35.752536Z",
     "start_time": "2024-11-03T20:42:10.939167Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Seed: 42 >\n",
      "  Test accuracy: 0.7940239310264587, Unfairness (EQOPP): 0.18542869620068902\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_type = 'ours'\n",
    "\n",
    "full_tests = []\n",
    "full_trains = []\n",
    "\n",
    "\"\"\" Find new data ratio for each (y, z)-class \"\"\"  \n",
    "w_new = find_w_cvxpy(w, corr, 0.1, 0.1)\n",
    "\n",
    "\"\"\" Find example weights according to the new weight \"\"\"  \n",
    "our_weights = correlation_reweighting(xz_train, y_train, z_train, w, w_new)\n",
    "\n",
    "\"\"\" Train models \"\"\"\n",
    "for seed in [42]:\n",
    "\n",
    "    print(\"< Seed: {} >\".format(seed))\n",
    "\n",
    "    # ---------------------\n",
    "    #  Initialize model, optimizer, and criterion\n",
    "    # ---------------------\n",
    "\n",
    "    useCuda = False\n",
    "    if useCuda:\n",
    "        model = LogisticRegression(xz_train.shape[1],1).cuda()\n",
    "    else:\n",
    "        model = LogisticRegression(xz_train.shape[1],1)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    model.apply(weights_init_normal)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.999))\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    losses = []\n",
    "    \n",
    "    \n",
    "    # ---------------------\n",
    "    #  Set data and batch sampler\n",
    "    # ---------------------\n",
    "    \n",
    "    if train_type == 'in-processing-only':\n",
    "        train_data = CustomDataset(xz_train, y_train, z_train)\n",
    "    else:\n",
    "        new_index = datasampling(xz_train, y_train, z_train, our_weights, seed = seed)\n",
    "        train_data = CustomDataset(xz_train[new_index], y_train[new_index], z_train[new_index])\n",
    "\n",
    "    sampler = FairBatch (model, train_data.x, train_data.y, train_data.z, batch_size = 100, alpha = alpha, target_fairness = 'eqopp', replacement = False, seed = seed)\n",
    "    train_loader = torch.utils.data.DataLoader (train_data, sampler=sampler, num_workers=0)\n",
    "\n",
    "    \n",
    "    # ---------------------\n",
    "    #  Model training\n",
    "    # ---------------------\n",
    "\n",
    "    for epoch in range(500):\n",
    "        print(epoch, end=\"\\r\")\n",
    "\n",
    "        tmp_loss = []\n",
    "\n",
    "        for batch_idx, (data, target, z) in enumerate (train_loader):\n",
    "            loss = run_epoch (model, data, target, optimizer, criterion)\n",
    "            tmp_loss.append(loss)\n",
    "\n",
    "        losses.append(sum(tmp_loss)/len(tmp_loss))\n",
    "    \n",
    "    tmp_test = test_model(model, xz_test, y_test, z_test)\n",
    "    full_tests.append(tmp_test)\n",
    "    \n",
    "    # print(\"  Test accuracy: {}, Unfairness (EQOPP): {}\".format(tmp_test['Acc'], tmp_test['EO_Y1_diff']))\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T20:42:35.773478Z",
     "start_time": "2024-11-03T20:42:35.761044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8480511515036163"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_digits = model(xz_test).detach().numpy()\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score((y_test + 1) / 2, 1/(1+np.exp(-pred_digits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T20:42:35.854354Z",
     "start_time": "2024-11-03T20:42:35.848228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1743009151590087"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# measure equal opportunity, i.e. difference in true positive rates for the two groups\n",
    "tpr_privileged = np.mean((pred_digits>0)[X_test.gender == 1])\n",
    "tpr_unprivileged = np.mean((pred_digits>0)[X_test.gender == 0])\n",
    "eq_opp = tpr_privileged - tpr_unprivileged\n",
    "eq_opp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T01:34:30.106770Z",
     "start_time": "2024-11-05T01:34:25.092274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Number of Data ----------\n",
      "Train data : 29361, Test data : 15060 \n",
      "------------------------------------\n",
      "0.020490211141231035 0.01843341145308941 0.21669868107558538\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "from argparse import Namespace\n",
    "from models import LogisticRegression, weights_init_normal\n",
    "from FairBatchSampler_Multiple import FairBatch, CustomDataset\n",
    "from utils import correlation_reweighting, datasampling, test_model\n",
    "\n",
    "import cvxopt\n",
    "import cvxpy as cp\n",
    "from cvxpy import OPTIMAL, Minimize, Problem, Variable, quad_form # Work in YJ kernel\n",
    "\n",
    "\n",
    "# from aif360.algorithms.preprocessing.lfr import LFR\n",
    "# from aif360.algorithms.preprocessing import Reweighing\n",
    "# from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression as SKLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../../\"))\n",
    "os.chdir('../../')\n",
    "from load_dataset import load\n",
    "\n",
    "from API_Design_a import MissingValueError, SamplingError, LabelError, Injector\n",
    "\n",
    "\n",
    "# create pattern function given subpopulation\n",
    "def create_pattern(col_list, lb_list, ub_list):\n",
    "    # Check if inputs are valid\n",
    "    try:\n",
    "        assert len(col_list) == len(lb_list) == len(ub_list)\n",
    "    except:\n",
    "        print(col_list, lb_list, ub_list)\n",
    "        raise SyntaxError\n",
    "\n",
    "    def pattern(data_X, data_y):\n",
    "        # Initialize a mask of all True values\n",
    "        mask = np.ones(len(data_X), dtype=bool)\n",
    "\n",
    "        # Iterate over each condition in col_list, lb_list, and ub_list\n",
    "        for col, lb, ub in zip(col_list, lb_list, ub_list):\n",
    "            if col == 'Y':\n",
    "                mask &= (data_y >= lb) & (data_y <= ub)\n",
    "            else:\n",
    "                mask &= (data_X[col] >= lb) & (data_X[col] <= ub)\n",
    "\n",
    "        # Convert Boolean mask to binary indicators (1 for True, 0 for False)\n",
    "        binary_indicators = mask.astype(int)\n",
    "\n",
    "        return binary_indicators\n",
    "\n",
    "    return pattern\n",
    "\n",
    "X_train, X_test, y_train, y_test = load('adult')\n",
    "# compute correlation between label and sensitive attribute using corrcoef\n",
    "corr = np.corrcoef(X_train.gender, y_train)[0, 1]\n",
    "# corr = 0.18\n",
    "\n",
    "pr_y_orig = sum(y_train == 1) / len(y_train)\n",
    "pr_z_orig = sum(X_train.gender == 1) / len(X_train)\n",
    "\n",
    "lb_list = [0, 6, 1]\n",
    "ub_list = [0, 7, 1]\n",
    "mv_pattern = create_pattern(['gender', 'workclass', 'Y'], lb_list, ub_list)\n",
    "mv_pattern_len = np.sum(mv_pattern(X_train, y_train))\n",
    "poi_ratio = 0.1\n",
    "\n",
    "mv_num = min(mv_pattern_len, int(poi_ratio*len(X_train)))\n",
    "mv_err = SamplingError(mv_pattern, mv_num / mv_pattern_len)\n",
    "injector = Injector(error_seq=[mv_err])\n",
    "X_train, y_train, _, _ = injector.inject(X_train, y_train, X_train, y_train, seed=0)\n",
    "\n",
    "pr_y_shifted = sum(y_train == 1) / len(y_train)\n",
    "pr_z_shifted = sum(X_train.gender == 1) / len(X_train)\n",
    "\n",
    "y_train = y_train.replace({0: -1, 1: 1})\n",
    "y_test = y_test.replace({0: -1, 1: 1})\n",
    "\n",
    "xz_train = X_train.copy()\n",
    "z_train = X_train.gender.copy()\n",
    "y_noise = y_train.copy()\n",
    "\n",
    "xz_test = X_test.copy()\n",
    "z_test = X_test.gender.copy()\n",
    "\n",
    "xz_train = torch.FloatTensor(xz_train.to_numpy())\n",
    "y_train = torch.FloatTensor(y_train.to_numpy())\n",
    "z_train = torch.FloatTensor(z_train.to_numpy())\n",
    "\n",
    "xz_test = torch.FloatTensor(xz_test.to_numpy())\n",
    "y_test = torch.FloatTensor(y_test.to_numpy())\n",
    "z_test = torch.FloatTensor(z_test.to_numpy())\n",
    "\n",
    "\n",
    "print(\"---------- Number of Data ----------\" )\n",
    "print(\n",
    "    \"Train data : %d, Test data : %d \"\n",
    "    % (len(y_train), len(y_test))\n",
    ")\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "w = np.array([sum((z_train==1)&(y_train==1))/len(y_train), sum((z_train==0)&(y_train==1))/len(y_train),\n",
    "              sum((z_train==1)&(y_train==-1))/len(y_train), sum((z_train==0)&(y_train==-1))/len(y_train)])\n",
    "alpha = 0.005 #\n",
    "\n",
    "# test robust algo\n",
    "def run_epoch(model, train_features, labels, optimizer, criterion):\n",
    "    \"\"\"Trains the model with the given train data.\n",
    "\n",
    "    Args:\n",
    "        model: A torch model to train.\n",
    "        train_features: A torch tensor indicating the train features.\n",
    "        labels: A torch tensor indicating the true labels.\n",
    "        optimizer: A torch optimizer.\n",
    "        criterion: A torch criterion.\n",
    "\n",
    "    Returns:\n",
    "        loss value.\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    label_predicted = model.forward(train_features)\n",
    "    loss = criterion((F.tanh(label_predicted.squeeze()) + 1) / 2, (labels.squeeze() + 1) / 2)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def find_w_cvxpy(w, corr, gamma1, gamma2):\n",
    "    \"\"\"Solves the SDP relaxation problem.\n",
    "\n",
    "    Args:\n",
    "        w: A list indicating the original data ratio for each (y, z)-class.\n",
    "        corr: A real number indicating the target correlation.\n",
    "        gamma1: A real number indicating the range of Pr(y) change\n",
    "        gamma2: A real number indicating the range of Pr(z) change\n",
    "\n",
    "    Returns:\n",
    "        solution for the optimization problem.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(w)\n",
    "    a = w[0]\n",
    "    b = w[1]\n",
    "    c = w[2]\n",
    "    d = w[3]\n",
    "    orig_corr = w[0] / (w[0] + w[2]) - w[1] / (w[1] + w[3])\n",
    "\n",
    "    P0 = np.array([[1, 0, 0, 0, -a], [0, 1, 0, 0, -b], [0, 0, 1, 0, -c], [0, 0, 0, 1, -d], [-a, -b, -c, -d, 0]])\n",
    "\n",
    "    P1 = np.array([[0, -corr / 2, 0, (1 - corr) / 2, 0], [-corr / 2, 0, (-1 - corr) / 2, 0, 0],\n",
    "                   [0, (-1 - corr) / 2, 0, -corr / 2, 0], [(1 - corr) / 2, 0, -corr / 2, 0, 0], [0, 0, 0, 0, 0]])\n",
    "\n",
    "    P2 = np.array([[0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [1, 1, 0, 0, 0]])\n",
    "    r2 = -2 * (a + b)\n",
    "\n",
    "    P3 = np.array([[0, 0, 0, 0, 1], [0, 0, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 0, 0], [1, 0, 1, 0, 0]])\n",
    "    r3 = -2 * (a + c)\n",
    "\n",
    "    P4 = np.array([[0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [1, 1, 1, 1, 0]])\n",
    "    r4 = -2 * 1\n",
    "\n",
    "    P5 = np.array([[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 1]])\n",
    "\n",
    "    X = cp.Variable((n + 1, n + 1), symmetric=True)\n",
    "\n",
    "    constraints = [X >> 0]\n",
    "    constraints = [\n",
    "        cp.trace(P1 @ X) == 0,\n",
    "        cp.trace(P2 @ X) + r2 <= gamma1,\n",
    "        cp.trace(P2 @ X) + r2 >= -gamma1,\n",
    "        cp.trace(P3 @ X) + r3 <= gamma2,\n",
    "        cp.trace(P3 @ X) + r3 >= -gamma2,\n",
    "        cp.trace(P4 @ X) + r4 == 0,\n",
    "        cp.trace(P5 @ X) == 1,\n",
    "        X >> 0\n",
    "    ]\n",
    "    prob = cp.Problem(cp.Minimize(cp.trace(P0 @ X)), constraints)\n",
    "\n",
    "    result = prob.solve()\n",
    "\n",
    "    x = X.value\n",
    "    x = x[:, -1][:-1]\n",
    "    return x\n",
    "\n",
    "\n",
    "# Set the train data\n",
    "train_data = CustomDataset(xz_train, y_noise, z_train)\n",
    "\n",
    "seeds = [42, 43, 44, 45, 46]\n",
    "\n",
    "fairshift_aucs = []\n",
    "fairshift_eos = []\n",
    "\n",
    "reweighing_aucs = []\n",
    "reweighing_eos = []\n",
    "\n",
    "lfr_aucs = []\n",
    "lfr_eos = []\n",
    "\n",
    "train_type = 'ours'\n",
    "\n",
    "full_tests = []\n",
    "full_trains = []\n",
    "\n",
    "\"\"\" Find new data ratio for each (y, z)-class \"\"\"\n",
    "gamma_y = abs(pr_y_shifted - pr_y_orig)\n",
    "gamma_z = abs(pr_z_shifted - pr_z_orig)\n",
    "w_new = find_w_cvxpy(w, corr, gamma_y, gamma_z)\n",
    "\n",
    "\"\"\" Find example weights according to the new weight \"\"\"\n",
    "our_weights = correlation_reweighting(xz_train, y_train, z_train, w, w_new)\n",
    "\n",
    "print(gamma_y, gamma_z, corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T01:51:45.570302Z",
     "start_time": "2024-11-05T01:49:05.637246Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Seed: 42 >\n",
      "3\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [00:31<02:07, 31.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Robust Algo:\n",
      "Test AUC: 0.8482211648267988, EO: 0.21663141311432146\n",
      "----------------------------------------------------------------------\n",
      "< Seed: 43 >\n",
      "3\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [01:03<01:35, 31.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Robust Algo:\n",
      "Test AUC: 0.8492338456414161, EO: 0.27060619163956723\n",
      "----------------------------------------------------------------------\n",
      "< Seed: 44 >\n",
      "3\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [01:36<01:04, 32.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Robust Algo:\n",
      "Test AUC: 0.8498160925009517, EO: 0.2116544074175835\n",
      "----------------------------------------------------------------------\n",
      "< Seed: 45 >\n",
      "3\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [02:08<00:32, 32.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Robust Algo:\n",
      "Test AUC: 0.849016225732775, EO: 0.2935370899168367\n",
      "----------------------------------------------------------------------\n",
      "< Seed: 46 >\n",
      "1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:39<00:00, 31.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498\r",
      "499\r",
      "----------------------------------------------------------------------\n",
      "Robust Algo:\n",
      "Test AUC: 0.8488676008755234, EO: 0.25622068590484337\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "FairShift:\n",
      "Test AUC: 0.849030985915493 +- 0.0005177387659632739\n",
      "EO: 0.2209025808226237 +- 0.03789849346114616\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train models \"\"\"\n",
    "for seed in tqdm(seeds):\n",
    "\n",
    "    print(\"< Seed: {} >\".format(seed))\n",
    "\n",
    "    # ---------------------\n",
    "    #  Initialize model, optimizer, and criterion\n",
    "    # ---------------------\n",
    "\n",
    "    useCuda = False\n",
    "    if useCuda:\n",
    "        model = LogisticRegression(xz_train.shape[1], 1).cuda()\n",
    "    else:\n",
    "        model = LogisticRegression(xz_train.shape[1], 1)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    model.apply(weights_init_normal)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.999))\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    # ---------------------\n",
    "    #  Set data and batch sampler\n",
    "    # ---------------------\n",
    "\n",
    "    if train_type == 'in-processing-only':\n",
    "        train_data = CustomDataset(xz_train, y_train, z_train)\n",
    "    else:\n",
    "        new_index = datasampling(xz_train, y_train, z_train, our_weights, seed=seed)\n",
    "        train_data = CustomDataset(xz_train[new_index], y_train[new_index], z_train[new_index])\n",
    "\n",
    "    sampler = FairBatch(model, train_data.x, train_data.y, train_data.z, batch_size=100, alpha=alpha,\n",
    "                        target_fairness='eqopp', replacement=False, seed=seed)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, sampler=sampler, num_workers=0)\n",
    "\n",
    "    # ---------------------\n",
    "    #  Model training\n",
    "    # ---------------------\n",
    "\n",
    "    for epoch in range(500):\n",
    "        print(epoch, end=\"\\r\")\n",
    "\n",
    "        tmp_loss = []\n",
    "\n",
    "        for batch_idx, (data, target, z) in enumerate(train_loader):\n",
    "            loss = run_epoch(model, data, target, optimizer, criterion)\n",
    "            tmp_loss.append(loss)\n",
    "\n",
    "        losses.append(sum(tmp_loss) / len(tmp_loss))\n",
    "\n",
    "    pred_digits = model(xz_test).detach().numpy()\n",
    "    idx_privileged = np.where((X_test.gender == 1).to_numpy() & (y_test==1).detach().numpy())[0]\n",
    "    tpr_privileged = np.mean((pred_digits > 0)[idx_privileged])\n",
    "    idx_protected = np.where((X_test.gender == 0).to_numpy() & (y_test==1).detach().numpy())[0]\n",
    "    tpr_protected = np.mean((pred_digits > 0)[idx_protected])\n",
    "    eq_opp = tpr_privileged - tpr_protected\n",
    "\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "    print('Robust Algo:')\n",
    "    print(f\"Test AUC: {roc_auc_score((y_test + 1) / 2, 1/(1+np.exp(-pred_digits)))}, EO: {eq_opp}\")\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "    fairshift_aucs.append(roc_auc_score((y_test + 1) / 2, 1/(1+np.exp(-pred_digits))))\n",
    "    fairshift_eos.append(eq_opp)\n",
    "\n",
    "\n",
    "# print mean and std\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print('FairShift:')\n",
    "print(f\"Test AUC: {np.mean(fairshift_aucs)} +- {np.std(fairshift_aucs)}\")\n",
    "print(f\"EO: {np.mean(fairshift_eos)} +- {np.std(fairshift_eos)}\")\n",
    "print(\"----------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T01:47:44.772234Z",
     "start_time": "2024-11-05T01:47:44.767705Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    2,     3,     5, ..., 15036, 15042, 15059])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T02:13:08.392683Z",
     "start_time": "2024-11-05T02:13:08.389593Z"
    }
   },
   "outputs": [],
   "source": [
    "from metrics import computeF1, computeFairness, computeAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T02:14:30.542394Z",
     "start_time": "2024-11-05T02:14:30.529108Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5221990703638404"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeF1(((y_test + 1) / 2).detach().numpy(), pred_digits>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T02:14:26.013972Z",
     "start_time": "2024-11-05T02:14:26.005976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.802058432934927"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeAccuracy(((y_test + 1) / 2).detach().numpy(), (pred_digits>0).astype(float).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T02:14:18.165508Z",
     "start_time": "2024-11-05T02:14:18.140053Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ..., False,  True,  True])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred_digits>0).astype(float).ravel()==((y_test + 1) / 2).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T02:13:29.052047Z",
     "start_time": "2024-11-05T02:13:29.008434Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1.,  ..., 0., 0., 1.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_test + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T02:10:30.461825Z",
     "start_time": "2024-11-05T02:10:30.038818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.25622069])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeFairness((pred_digits>0).astype(float), X_test, (y_test + 1) / 2, 1, 'adult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T02:10:43.403301Z",
     "start_time": "2024-11-05T02:10:42.961998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.19342827])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeFairness((pred_digits>0).astype(float), X_test, (y_test + 1) / 2, 0, 'adult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

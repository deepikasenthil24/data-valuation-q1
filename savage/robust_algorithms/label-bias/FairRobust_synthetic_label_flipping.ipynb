{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fair and Robust Sample Selection on the Synthetic Dataset\n",
    "## With Label Flipping\n",
    "\n",
    "#### This Jupyter Notebook simulates the proposed fair and robust sample selection on the synthetic data.\n",
    "#### We use two fairness metrics: equalized odds and demographic parity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T03:34:16.205100Z",
     "start_time": "2024-11-03T03:34:16.068867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lodino/Desktop/work/workspace/newest_data_error/data-err-experiment/robust_algorithms/fair-robust-selection\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T03:34:16.979761Z",
     "start_time": "2024-11-03T03:34:16.317013Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import copy\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import torch\n",
    "\n",
    "from models import LogisticRegression, weights_init_normal, test_model\n",
    "from FairRobustSampler import FairRobust, CustomDataset\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T03:34:17.166207Z",
     "start_time": "2024-11-03T03:34:17.062556Z"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('../../')\n",
    "from load_dataset import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process the data\n",
    "In the synthetic_data directory, there are a total of 11 numpy files including training data (both clean and noisy), validation data, and test data. Note that the validation data is utilized for another method in the paper (i.e., FR-Train), so the data is not used in this program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T03:30:24.467688Z",
     "start_time": "2024-11-03T03:30:24.431025Z"
    }
   },
   "outputs": [],
   "source": [
    "from API_Design_a import MissingValueError, SamplingError, LabelError, Injector\n",
    "# create pattern function given subpopulation\n",
    "def create_pattern(col_list, lb_list, ub_list):\n",
    "    # Check if inputs are valid\n",
    "    try:\n",
    "        assert len(col_list) == len(lb_list) == len(ub_list)\n",
    "    except:\n",
    "        print(col_list, lb_list, ub_list)\n",
    "        raise SyntaxError\n",
    "\n",
    "    def pattern(data_X, data_y):\n",
    "        # Initialize a mask of all True values\n",
    "        mask = np.ones(len(data_X), dtype=bool)\n",
    "\n",
    "        # Iterate over each condition in col_list, lb_list, and ub_list\n",
    "        for col, lb, ub in zip(col_list, lb_list, ub_list):\n",
    "            if col == 'Y':\n",
    "                mask &= (data_y >= lb) & (data_y <= ub)\n",
    "            else:\n",
    "                mask &= (data_X[col] >= lb) & (data_X[col] <= ub)\n",
    "\n",
    "        # Convert Boolean mask to binary indicators (1 for True, 0 for False)\n",
    "        binary_indicators = mask.astype(int)\n",
    "        \n",
    "        return binary_indicators\n",
    "\n",
    "    return pattern\n",
    "\n",
    "\n",
    "# lb_list = [0, 0]\n",
    "# ub_list = [0, 0]\n",
    "# X_train, X_test, y_train, y_test = load(dataset)\n",
    "# X_train_orig, X_test_orig = X_train.copy(), X_test.copy()\n",
    "\n",
    "# X_train_orig.reset_index(drop=True, inplace=True)\n",
    "# X_test_orig.reset_index(drop=True, inplace=True)\n",
    "# y_train.reset_index(drop=True, inplace=True)\n",
    "# y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# mv_pattern = create_pattern(['race', 'Y'], lb_list, ub_list)\n",
    "# mv_pattern_len = np.sum(mv_pattern(X_train_orig, y_train))\n",
    "# mv_num = min(mv_pattern_len, int(0.4*len(X_train_orig)))\n",
    "# mv_err = MissingValueError(list(X_train_orig.columns).index('race'), mv_pattern, mv_num / mv_pattern_len)\n",
    "\n",
    "# injecter = Injector(error_seq=[mv_err])\n",
    "# dirty_X_train_orig, dirty_y_train, _, _ = injecter.inject(X_train_orig.copy(), y_train.copy(), \n",
    "#                                                           X_train_orig, y_train, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T03:30:26.529232Z",
     "start_time": "2024-11-03T03:30:25.174056Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load('adult')\n",
    "\n",
    "lb_list = [6, 1, 0]\n",
    "ub_list = [8, 1, 0]\n",
    "\n",
    "mv_pattern = create_pattern(['education', 'gender', 'Y'], lb_list, ub_list)\n",
    "mv_pattern_len = np.sum(mv_pattern(X_train, y_train))\n",
    "poi_ratio = 0.1\n",
    "mv_num = min(mv_pattern_len, int(poi_ratio*len(X_train)))\n",
    "mv_err = LabelError(mv_pattern, mv_num / mv_pattern_len)\n",
    "injector = Injector(error_seq=[mv_err])\n",
    "X_train, y_train, _, _ = injector.inject(X_train, y_train, X_train, y_train, seed=0)\n",
    "\n",
    "y_train = y_train.replace({0: -1, 1: 1})\n",
    "y_test = y_test.replace({0: -1, 1: 1})\n",
    "\n",
    "xz_train = X_train.copy()\n",
    "z_train = X_train.gender.copy()\n",
    "y_noise = y_train.copy()\n",
    "\n",
    "xz_test = X_test.copy()\n",
    "z_test = X_test.gender.copy()\n",
    "\n",
    "# y_train = y_train*2-1\n",
    "# y_test = y_test*2-1\n",
    "\n",
    "xz_train = torch.FloatTensor(xz_train.to_numpy())\n",
    "y_train = torch.FloatTensor(y_train.to_numpy())\n",
    "z_train = torch.FloatTensor(z_train.to_numpy())\n",
    "\n",
    "y_noise = torch.FloatTensor(y_noise.to_numpy())\n",
    "\n",
    "xz_test = torch.FloatTensor(xz_test.to_numpy())\n",
    "y_test = torch.FloatTensor(y_test.to_numpy())\n",
    "z_test = torch.FloatTensor(z_test.to_numpy())\n",
    "\n",
    "# os.chdir('robust_algorithms/fair-robust-selection')\n",
    "# xz_train = np.load('./synthetic_data/xz_train.npy')\n",
    "# y_train = np.load('./synthetic_data/y_train.npy')\n",
    "# z_train = np.load('./synthetic_data/z_train.npy')\n",
    "# \n",
    "# y_noise = np.load('./synthetic_data/y_noise_general.npy') # Labels with the general label flipping (details are in the paper)\n",
    "# poi_ratio = 0.0\n",
    "# \n",
    "# xz_test = np.load('./synthetic_data/xz_test.npy')\n",
    "# y_test = np.load('./synthetic_data/y_test.npy') \n",
    "# z_test = np.load('./synthetic_data/z_test.npy')\n",
    "# \n",
    "# xz_train = torch.FloatTensor(xz_train)\n",
    "# y_train = torch.FloatTensor(y_train)\n",
    "# z_train = torch.FloatTensor(z_train)\n",
    "# \n",
    "# y_noise = torch.FloatTensor(y_noise)\n",
    "# \n",
    "# xz_test = torch.FloatTensor(xz_test)\n",
    "# y_test = torch.FloatTensor(y_test)\n",
    "# z_test = torch.FloatTensor(z_test)\n",
    "# os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T03:30:27.401143Z",
     "start_time": "2024-11-03T03:30:27.383111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Number of Data ----------\n",
      "Train data : 30162, Test data : 15060 \n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"---------- Number of Data ----------\" )\n",
    "print(\n",
    "    \"Train data : %d, Test data : %d \"\n",
    "    % (len(y_train), len(y_test))\n",
    ")       \n",
    "print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T03:30:29.337683Z",
     "start_time": "2024-11-03T03:30:29.327365Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_epoch(model, train_features, labels, optimizer, criterion):\n",
    "    \"\"\"Trains the model with the given train data.\n",
    "\n",
    "    Args:\n",
    "        model: A torch model to train.\n",
    "        train_features: A torch tensor indicating the train features.\n",
    "        labels: A torch tensor indicating the true labels.\n",
    "        optimizer: A torch optimizer.\n",
    "        criterion: A torch criterion.\n",
    "\n",
    "    Returns:\n",
    "        loss values.\n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    label_predicted = model.forward(train_features)\n",
    "    loss  = criterion((F.tanh(label_predicted.squeeze())+1)/2, (labels.squeeze()+1)/2)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 1. Fair and Robust Sample Selection w.r.t. Equalized Odds\n",
    "### The results are in the Experiments section of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T03:13:21.434363Z",
     "start_time": "2024-11-03T03:11:02.556378Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Test accuracy: 0.756905734539032, EO disparity: 0.11056257194221342\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "full_tests = []\n",
    "\n",
    "parameters = Namespace(warm_start=100, tau=1-poi_ratio, alpha = 0.001, batch_size = 100)\n",
    "\n",
    "# Set the train data\n",
    "train_data = CustomDataset(xz_train, y_noise, z_train)\n",
    "\n",
    "seed  = 0\n",
    "# ---------------------\n",
    "#  Initialize model, optimizer, and criterion\n",
    "# ---------------------\n",
    "\n",
    "model = LogisticRegression(xz_train.shape[1],1)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "model.apply(weights_init_normal)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.999))\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "losses = []\n",
    "\n",
    "# ---------------------\n",
    "#  Define FairRobust and DataLoader\n",
    "# ---------------------\n",
    "\n",
    "sampler = FairRobust (model, train_data.x, train_data.y, train_data.z, target_fairness = 'eqodds', parameters = parameters, replacement = False, seed = seed)\n",
    "train_loader = torch.utils.data.DataLoader (train_data, sampler=sampler, num_workers=0)\n",
    "\n",
    "# ---------------------\n",
    "#  Model training\n",
    "# ---------------------\n",
    "for epoch in range(400):\n",
    "    print(epoch, end=\"\\r\")\n",
    "    \n",
    "    tmp_loss = []\n",
    "    \n",
    "    for batch_idx, (data, target, z) in enumerate (train_loader):\n",
    "        loss = run_epoch (model, data, target, optimizer, criterion)\n",
    "        tmp_loss.append(loss)\n",
    "    \n",
    "    losses.append(sum(tmp_loss)/len(tmp_loss))\n",
    "\n",
    "tmp_test = test_model(model, xz_test, y_test, z_test)\n",
    "full_tests.append(tmp_test)\n",
    "\n",
    "# print(\"  Test accuracy: {}, EO disparity: {}\".format(tmp_test['Acc'], tmp_test['EqOdds_diff']))\n",
    "print(\"----------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T03:13:21.499361Z",
     "start_time": "2024-11-03T03:13:21.474858Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8280654739246289"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_digits = model(xz_test).detach().numpy()\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score((y_test + 1) / 2, 1/(1+np.exp(-pred_digits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T03:13:21.602348Z",
     "start_time": "2024-11-03T03:13:21.592369Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2694776165494446"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# measure equal opportunity, i.e. difference in true positive rates for the two groups\n",
    "tpr_privileged = np.mean((pred_digits>0)[X_test.gender == 1])\n",
    "tpr_unprivileged = np.mean((pred_digits>0)[X_test.gender == 0])\n",
    "eq_opp = tpr_privileged - tpr_unprivileged\n",
    "eq_opp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIF360 fairness algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T03:34:22.054251Z",
     "start_time": "2024-11-03T03:34:21.839503Z"
    }
   },
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "initialization of _internal failed without raising an exception",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mSystemError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maif360\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01malgorithms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Reweighing\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maif360\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BinaryLabelDataset\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/aif360/algorithms/__init__.py:1\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maif360\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01malgorithms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransformer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Transformer, addmetadata\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/aif360/algorithms/transformer.py:4\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mabc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m abstractmethod\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfunctools\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m wraps\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maif360\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maif360\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdecorating_metaclass\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ApplyDecorator\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# TODO: Use sklearn.exceptions.NotFittedError instead?\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/aif360/datasets/__init__.py:13\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maif360\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmeps_dataset_panel21_fy2016\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MEPSDataset21\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maif360\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mregression_dataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RegressionDataset\n\u001B[0;32m---> 13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maif360\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlaw_school_gpa_dataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LawSchoolGPADataset\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/aif360/datasets/law_school_gpa_dataset.py:4\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maif360\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RegressionDataset\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtempeh\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfigurations\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtc\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mLawSchoolGPADataset\u001B[39;00m(RegressionDataset):\n\u001B[1;32m      8\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Law School GPA dataset.\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \n\u001B[1;32m     10\u001B[0m \u001B[38;5;124;03m    See https://github.com/microsoft/tempeh for details.\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tempeh/configurations.py:6\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright (c) Microsoft Corporation. All rights reserved.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Licensed under the MIT License.\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03m\"\"\"Holds all the datasets and models that are automatically enqueued.\"\"\"\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtempeh\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SKLearnPerformanceDatasetWrapper, UCIPerformanceDatasetWrapper, \\\n\u001B[1;32m      7\u001B[0m     BlobPerformanceDatasetWrapper, CompasPerformanceDatasetWrapper, \\\n\u001B[1;32m      8\u001B[0m     SEAPHEPerformanceDatasetWrapper\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtempeh\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RBMSVMModelWrapper, LinearSVMModelWrapper, LogisticModelWrapper, \\\n\u001B[1;32m     10\u001B[0m     RidgeModelWrapper, DecisionTreeClassifierWrapper, RandomForestClassifierWrapper, \\\n\u001B[1;32m     11\u001B[0m     RandomForestRegressorWrapper, PytorchMulticlassClassifierWrapper, \\\n\u001B[1;32m     12\u001B[0m     PytorchBinaryClassifierWrapper, PytorchRegressionWrapper, XGBoostClassifierWrapper, \\\n\u001B[1;32m     13\u001B[0m     XGBoostRegressorWrapper, LightGBMClassifierWrapper, LightGBMRegressorWrapper, \\\n\u001B[1;32m     14\u001B[0m     KerasMulticlassClassifierWrapper, KerasBinaryClassifierWrapper, KerasRegressionWrapper\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtempeh\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconstants\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DatasetSizes\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tempeh/datasets/__init__.py:5\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright (c) Microsoft Corporation. All rights reserved.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Licensed under the MIT License.\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msk_datasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SKLearnPerformanceDatasetWrapper\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01muci_datasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m UCIPerformanceDatasetWrapper\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mblob_datasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BlobPerformanceDatasetWrapper\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompas_datasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CompasPerformanceDatasetWrapper\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tempeh/datasets/uci_datasets.py:8\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase_wrapper\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BasePerformanceDatasetWrapper\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01muci_dataset_cleaner\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m bank_data_parser, bank_data_additional_parser, car_eval_parser, \\\n\u001B[1;32m      9\u001B[0m     adult_data_parser\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtempeh\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconstants\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FeatureType, Tasks, DataTypes, UCIDatasets, ClassVars  \u001B[38;5;66;03m# noqa\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mUCIPerformanceDatasetWrapper\u001B[39;00m(BasePerformanceDatasetWrapper):\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tempeh/datasets/uci_dataset_cleaner.py:6\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# TODO: remove shap dependency\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mshap\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01murllib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m urlretrieve\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/shap/__init__.py:12\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (sys\u001B[38;5;241m.\u001B[39mversion_info \u001B[38;5;241m<\u001B[39m (\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m0\u001B[39m)):\n\u001B[1;32m     10\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAs of version 0.29.0 shap only supports Python 3 (not 2)!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_explanation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Explanation, Cohorts\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# explainers\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexplainers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_explainer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Explainer\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/shap/_explanation.py:12\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mslicer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Slicer, Alias, Obj\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# from ._order import Order\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_general\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OpChain\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# slicer confuses pylint...\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# pylint: disable=no-member\u001B[39;00m\n\u001B[1;32m     18\u001B[0m op_chain_root \u001B[38;5;241m=\u001B[39m OpChain(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshap.Explanation\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/shap/utils/__init__.py:1\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_clustering\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m hclust_ordering, partition_tree, partition_tree_shuffle, delta_minimization_order, hclust\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_general\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m approximate_interactions, potential_interactions, sample, safe_isinstance, assert_import, record_import_error\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_general\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m shapley_coefficients, convert_name, format_value, ordinal_str, OpChain\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/shap/utils/_clustering.py:4\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msp\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspatial\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistance\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pdist\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumba\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m jit\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numba/__init__.py:43\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumba\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdecorators\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (cfunc, generated_jit, jit, njit, stencil,\n\u001B[1;32m     40\u001B[0m                                    jit_module)\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# Re-export vectorize decorators and the thread layer querying function\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumba\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mufunc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (vectorize, guvectorize, threading_layer,\n\u001B[1;32m     44\u001B[0m                             get_num_threads, set_num_threads)\n\u001B[1;32m     46\u001B[0m \u001B[38;5;66;03m# Re-export Numpy helpers\u001B[39;00m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumba\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnumpy_support\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m carray, farray, from_dtype\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numba/np/ufunc/__init__.py:3\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# -*- coding: utf-8 -*-\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumba\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mufunc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdecorators\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Vectorize, GUVectorize, vectorize, guvectorize\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumba\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mufunc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_internal\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PyUFunc_None, PyUFunc_Zero, PyUFunc_One\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumba\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mufunc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _internal, array_exprs\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numba/np/ufunc/decorators.py:3\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01minspect\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumba\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mufunc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _internal\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumba\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mufunc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparallel\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ParallelUFuncBuilder, ParallelGUFuncBuilder\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumba\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mregistry\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TargetRegistry\n",
      "\u001B[0;31mSystemError\u001B[0m: initialization of _internal failed without raising an exception"
     ]
    }
   ],
   "source": [
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "import pandas as pd\n",
    "\n",
    "# use the same dataset\n",
    "privileged_groups = [{'gender': 1}]\n",
    "unprivileged_groups = [{'gender': 0}]\n",
    "X_train_reweighed = BinaryLabelDataset(df=pd.concat([X_train, pd.Series(y_train.detach().numpy(), name='Y')], axis=1), \n",
    "                                       label_names=['Y'], protected_attribute_names=['gender'],\n",
    "                                       favorable_label=1, unfavorable_label=-1)\n",
    "\n",
    "RW = Reweighing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "RW.fit(X_train_reweighed)\n",
    "X_train_reweighed = RW.transform(X_train_reweighed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-03T03:29:52.887Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as SKLR\n",
    "clf = SKLR(random_state=42)\n",
    "clf.fit(X_train, y_train, sample_weight=X_train_reweighed.instance_weights)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# measure equal opportunity\n",
    "tpr_privileged = np.mean((y_pred>0.5)[X_test.gender == 1])\n",
    "tpr_unprivileged = np.mean((y_pred>0.5)[X_test.gender == 0])\n",
    "eq_opp = tpr_privileged - tpr_unprivileged\n",
    "\n",
    "eq_opp, roc_auc_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-03T03:20:52.002889Z"
    }
   },
   "outputs": [],
   "source": [
    "from aif360.algorithms.preprocessing.lfr import LFR\n",
    "\n",
    "# use the same dataset\n",
    "privileged_groups = [{'gender': 1}]\n",
    "unprivileged_groups = [{'gender': 0}]\n",
    "X_train_wrapped = BinaryLabelDataset(df=pd.concat([X_train, pd.Series(y_train.detach().numpy(), name='Y')], axis=1), \n",
    "                                     label_names=['Y'], protected_attribute_names=['gender'],\n",
    "                                     favorable_label=1, unfavorable_label=-1)\n",
    "X_train_wrapped = BinaryLabelDataset(df=pd.concat([X_train, pd.Series(y_train.detach().numpy(), name='Y')], axis=1), \n",
    "                                     label_names=['Y'], protected_attribute_names=['gender'],\n",
    "                                     favorable_label=1, unfavorable_label=-1)\n",
    "preproc = LFR(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups, k=10, Ax=0.1, Ay=2.0, Az=1.0, verbose=1, seed=1)\n",
    "preproc.fit(X_train_wrapped, maxiter=3000, maxfun=3000)\n",
    "X_train_transformed = preproc.transform(X_train_wrapped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SKLR(random_state=42)\n",
    "clf.fit(X_train_transformed.to_dataframe()[0], y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# measure equal opportunity\n",
    "tpr_privileged = np.mean((y_pred>0.5)[X_test.gender == 1])\n",
    "tpr_unprivileged = np.mean((y_pred>0.5)[X_test.gender == 0])\n",
    "eq_opp = tpr_privileged - tpr_unprivileged\n",
    "\n",
    "eq_opp, roc_auc_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fair and Robust Sample Selection w.r.t. Demographic Parity\n",
    "### The results are in the Experiments section of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Seed: 0 >\n",
      "  Test accuracy: 0.7190000414848328, DP disparity: 0.005151515151515129\n",
      "----------------------------------------------------------------------\n",
      "< Seed: 1 >\n",
      "  Test accuracy: 0.7200000286102295, DP disparity: 0.00615151515151513\n",
      "----------------------------------------------------------------------\n",
      "< Seed: 2 >\n",
      "  Test accuracy: 0.7210000157356262, DP disparity: 0.00410101010101005\n",
      "----------------------------------------------------------------------\n",
      "< Seed: 3 >\n",
      "  Test accuracy: 0.7200000286102295, DP disparity: 0.00615151515151513\n",
      "----------------------------------------------------------------------\n",
      "< Seed: 4 >\n",
      "  Test accuracy: 0.718000054359436, DP disparity: 0.0072020202020202095\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "full_tests = []\n",
    "\n",
    "parameters = Namespace(warm_start=100, tau=1-poi_ratio, alpha = 0.001, batch_size = 100)\n",
    "\n",
    "# Set the train data\n",
    "train_data = CustomDataset(xz_train, y_noise, z_train)\n",
    "\n",
    "seeds = [0,1,2,3,4]\n",
    "\n",
    "for seed in seeds:\n",
    "    \n",
    "    print(\"< Seed: {} >\".format(seed))\n",
    "    \n",
    "    # ---------------------\n",
    "    #  Initialize model, optimizer, and criterion\n",
    "    # ---------------------\n",
    "    \n",
    "    model = LogisticRegression(3,1).cuda()\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    model.apply(weights_init_normal)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.999))\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    losses = []\n",
    "    \n",
    "    # ---------------------\n",
    "    #  Define FairRobust and DataLoader\n",
    "    # ---------------------\n",
    "\n",
    "    sampler = FairRobust (model, train_data.x, train_data.y, train_data.z, target_fairness = 'dp', parameters = parameters, replacement = False, seed = seed)\n",
    "    train_loader = torch.utils.data.DataLoader (train_data, sampler=sampler, num_workers=0)\n",
    "\n",
    "    # ---------------------\n",
    "    #  Model training\n",
    "    # ---------------------\n",
    "    for epoch in range(400):\n",
    "        print(epoch, end=\"\\r\")\n",
    "        \n",
    "        tmp_loss = []\n",
    "        \n",
    "        for batch_idx, (data, target, z) in enumerate (train_loader):\n",
    "            loss = run_epoch (model, data, target, optimizer, criterion)\n",
    "            tmp_loss.append(loss)\n",
    "            \n",
    "        losses.append(sum(tmp_loss)/len(tmp_loss))\n",
    "        \n",
    "    tmp_test = test_model(model, xz_test, y_test, z_test)\n",
    "    full_tests.append(tmp_test)\n",
    "    \n",
    "    print(\"  Test accuracy: {}, DP disparity: {}\".format(tmp_test['Acc'], tmp_test['DP_diff']))\n",
    "    print(\"----------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy (avg): 0.7196000337600708\n",
      "DP disparity  (avg): 0.00575151515151513\n"
     ]
    }
   ],
   "source": [
    "tmp_acc = []\n",
    "tmp_dp = []\n",
    "for i in range(len(seeds)):\n",
    "    tmp_acc.append(full_tests[i]['Acc'])\n",
    "    tmp_dp.append(full_tests[i]['DP_diff'])\n",
    "\n",
    "print(\"Test accuracy (avg): {}\".format(sum(tmp_acc)/len(tmp_acc)))\n",
    "print(\"DP disparity  (avg): {}\".format(sum(tmp_dp)/len(tmp_dp)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

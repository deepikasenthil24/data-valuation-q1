{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7241971f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T05:54:25.570208Z",
     "start_time": "2024-02-26T05:54:25.565612Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/albertxu/miniconda3/envs/diffprep/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import tqdm\n",
    "import optuna\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from load_dataset import load\n",
    "from classifier import *\n",
    "from utils import *\n",
    "from metrics import *  # include fairness and corresponding derivatives\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.metrics import mutual_info_score, auc, roc_curve, roc_auc_score, f1_score, accuracy_score\n",
    "from scipy.stats import wasserstein_distance\n",
    "from optuna.samplers import *\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adam\n",
    "from diffprep.prep_space import space\n",
    "from diffprep.experiment.diffprep_experiment import DiffPrepExperiment\n",
    "from diffprep.pipeline.diffprep_fix_pipeline import DiffPrepFixPipeline\n",
    "from diffprep.trainer.diffprep_trainer import DiffPrepSGD\n",
    "from diffprep.model import LogisticRegression\n",
    "from diffprep.experiment.experiment_utils import min_max_normalize, set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b468921",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load('adult')\n",
    "X_train_orig, X_test_orig = X_train.copy(), X_test.copy()\n",
    "nan_indices = np.random.choice(X_train.index, 10000, replace=False)\n",
    "X_train.loc[nan_indices, 'education'] = np.nan\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.5, random_state=42\n",
    ")\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f02cf366",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"num_epochs\": 2000,\n",
    "    \"batch_size\": 512,\n",
    "    \"device\": \"cpu\",\n",
    "    \"model_lr\": 0.01,\n",
    "    # \"model_lr\": [0.01],\n",
    "    \"weight_decay\": 0,\n",
    "    \"model\": \"log\",\n",
    "    \"train_seed\": 1,\n",
    "    \"split_seed\": 1,\n",
    "    \"method\": \"diffprep_fix\",\n",
    "    \"save_model\": True,\n",
    "    \"logging\": False,\n",
    "    \"no_crash\": False,\n",
    "    \"patience\": 3,\n",
    "    \"momentum\": 0.9,\n",
    "    \"prep_lr\": None,\n",
    "    \"temperature\": 0.1,\n",
    "    \"grad_clip\": None,\n",
    "    \"pipeline_update_sample_size\": 512,\n",
    "    \"init_method\": \"default\",\n",
    "    \"diff_method\": \"num_diff\",\n",
    "    \"sample\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d67d3223",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (15081, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:05<00:00,  5.95it/s, next_eval_time=16s, tr_loss=0.408, val_loss=0.411]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_val, X_test = min_max_normalize(X_train, X_val, X_test)\n",
    "params[\"patience\"] = 10\n",
    "params[\"num_epochs\"] = 30\n",
    "set_random_seed(params)\n",
    "prep_pipeline = DiffPrepFixPipeline(space, temperature=params[\"temperature\"],\n",
    "                                             use_sample=params[\"sample\"],\n",
    "                                             diff_method=params[\"diff_method\"],\n",
    "                                             init_method=params[\"init_method\"])\n",
    "prep_pipeline.init_parameters(X_train, X_val, X_test)\n",
    "print(\"Train size: ({}, {})\".format(X_train.shape[0], prep_pipeline.out_features))\n",
    "\n",
    "# model\n",
    "input_dim = prep_pipeline.out_features\n",
    "output_dim = len(set(y_train.values.ravel()))\n",
    "\n",
    "# model = TwoLayerNet(input_dim, output_dim)\n",
    "set_random_seed(params)\n",
    "model = LogisticRegression(input_dim, output_dim)\n",
    "model = model.to(params[\"device\"])\n",
    " # loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "model_optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=params[\"model_lr\"],\n",
    "    weight_decay=params[\"weight_decay\"],\n",
    "    momentum=params[\"momentum\"]\n",
    ")\n",
    "\n",
    "if params[\"prep_lr\"] is None:\n",
    "    prep_lr = params[\"model_lr\"]\n",
    "else:\n",
    "    prep_lr = params[\"prep_lr\"]\n",
    "\n",
    "prep_pipeline_optimizer = torch.optim.Adam(\n",
    "    prep_pipeline.parameters(),\n",
    "    lr=prep_lr,\n",
    "    betas=(0.5, 0.999),\n",
    "    weight_decay=params[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "# scheduler\n",
    "# model_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model_optimizer, patience=patience, factor=0.1, threshold=0.001)\n",
    "prep_pipeline_scheduler = None\n",
    "model_scheduler = None\n",
    "\n",
    "if params[\"logging\"]:\n",
    "    logger = SummaryWriter()\n",
    "else:\n",
    "    logger = None\n",
    "\n",
    "diff_prep = DiffPrepSGD(prep_pipeline, model, loss_fn, model_optimizer, prep_pipeline_optimizer,\n",
    "            model_scheduler, prep_pipeline_scheduler, params, writer=logger)\n",
    "\n",
    "result, best_model = diff_prep.fit(X_train, y_train, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66d835b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = None\n",
    "best_model = None\n",
    "best_logger = None\n",
    "best_val_loss = float(\"inf\")\n",
    "best_params = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd071437",
   "metadata": {},
   "outputs": [],
   "source": [
    "if result[\"best_val_loss\"] < best_val_loss:\n",
    "    best_val_loss = result[\"best_val_loss\"]\n",
    "    best_result = result\n",
    "    best_model = model\n",
    "    best_logger = logger\n",
    "    best_params = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fdf6b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 29,\n",
       " 'best_val_loss': 0.4110908508300781,\n",
       " 'best_tr_acc': 0.7972283005105762,\n",
       " 'best_val_acc': 0.7966315231085471,\n",
       " 'best_test_acc': 0.8083001328021249,\n",
       " 'best_test_auc': 0.8554021221926151}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95174e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.8083\n",
      "AUC: 0.8554\n"
     ]
    }
   ],
   "source": [
    "auc_score = best_result[\"best_test_auc\"]\n",
    "accuracy = best_result[\"best_test_acc\"]\n",
    "\n",
    "print(f\"ACC: {accuracy:.4f}\")\n",
    "print(f\"AUC: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af1fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffprep",
   "language": "python",
   "name": "diffprep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
